# Distributed and Parallel ML Pipeline for Binary Classification
This project showcases the development of an optimized machine learning pipeline for binary classification using structured data. It compares the performance of traditional and accelerated approaches, including multi-core CPU execution, distributed systems (Dask + Coiled), and GPU acceleration (TensorFlow). The main models used are XGBoost and Neural Networks.

## ğŸ¯ Objectives

- Maximize model accuracy  
- Minimize training time (by at least 70%)  
- Leverage GPU acceleration, multi-threading, and distributed systems  

---

## ğŸ”„ Preprocessing Pipeline

- **Duplicate Removal**  
- **Missing Value Imputation**  
  - Categorical: Mode  
  - Numerical: Mean  
- **Outlier Detection & Removal**  
- **One-Hot Encoding** for categorical variables  
- **StandardScaler Normalization**

---

## ğŸ§  Modeling Approaches

### ğŸ”¸ Model 1: XGBoost Classifier
- **Baseline**: Single-core training with `XGBClassifier`
- **Optimized**: Distributed training using `dask_ml.xgboost` with Coiled clusters

### ğŸ”¹ Model 2: Neural Network (TensorFlow)
- **Baseline**: Sequential model on single-core CPU
- **Multi-Core**: Thread-parallel training using TensorFlowâ€™s CPU optimizations
- **GPU**: Final model trained on NVIDIA GPU for speed and improved convergence

---

## ğŸ“Š Evaluation Metrics

- **Accuracy**
- **F1 Score**
- **Confusion Matrix**
- **Processing Time (Seconds)**

---

## ğŸ“ˆ Benchmark Results

| Model Variant                  | Avg. Processing Time Reduction |
|-------------------------------|-------------------------------|
| XGBoost (CPU â†’ Dask)          | 91%                          |
| Neural Network (1-Core â†’ GPU) | 70%                          |
| Neural Network (1 â†’ Multi)    | 39%                          |
| Neural Network (Multi â†’ GPU)  | 45%                          |

> All model variants achieved over **60% accuracy**.

---

## âš–ï¸ Comparative Analysis

### XGBoost
- âœ… Great for structured/tabular data  
- âœ… Scales efficiently with Dask  
- âŒ Less expressive for complex patterns

### Neural Network
- âœ… Highly expressive with GPU acceleration  
- âŒ Computationally expensive on CPU  
- âŒ Requires hyperparameter tuning

---

## ğŸ§ª Experimental Setup

### Hardware
- CPU: Intel-based
- GPU: NVIDIA P100 (Kaggle)

### Software
- Python 3.13  
- Libraries:
  - `scikit-learn`
  - `xgboost`
  - `tensorflow`
  - `dask`, `coiled`
  - `pandas`, `numpy`

### Environments
- Local Jupyter Notebook  
- Kaggle Notebook  
- Coiled Cloud Cluster (for Dask)

---

## ğŸš€ Future Work

- Hyperparameter optimization via Ray Tune  
- Deploy as an API using FastAPI  
- Extend to multi-class classification

---

## ğŸ“œ License

This project is for academic purposes. Contact the authors for reuse.

---

